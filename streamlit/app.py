import streamlit as st
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import folium
from streamlit_folium import st_folium
import joblib

st.set_page_config(page_title="ğŸ“‰ íì—… ìœ„í—˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ", layout="wide")
plt.rcParams['font.family'] = 'Nanum Gothic'
plt.rcParams['axes.unicode_minus'] = False

@st.cache_resource
def load_code_name_dicts():
    return joblib.load("SKN13-2nd-4TEAM/ì‚°ì¶œë¬¼/best/dict_trans.joblib")

code_name_dicts = load_code_name_dicts()

@st.cache_data
def load_model_and_metadata(path):
    data = joblib.load(path)
    model = data.get('model', data)
    encoders = data.get('encoders', {})
    features = data.get('features', [])
    metrics = data.get('metrics', {})
    return model, encoders, features, metrics

def preprocess(df, encoder_dict):
    df = df.copy()
    for col in df.select_dtypes(include='object').columns:
        le = encoder_dict.get(col)
        if le:
            df[col] = le.transform(df[col])
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.fillna(0, inplace=True)
    scaler = StandardScaler()
    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
    return df_scaled

def normalize_colname(col):
    return col.lower().replace('_','').replace(' ','')

def align_columns(df, expected_cols):
    df_cols = df.columns.tolist()
    expected_norm = {normalize_colname(c): c for c in expected_cols}
    col_map = {}
    for col in df_cols:
        norm_col = normalize_colname(col)
        if norm_col in expected_norm:
            col_map[col] = expected_norm[norm_col]
    df_renamed = df.rename(columns=col_map)
    return df_renamed

def add_korean_names(df):
    df = df.copy()
    if 'ìƒê¶Œêµ¬ë¶„ì½”ë“œ' in df.columns:
        df['ìƒê¶Œêµ¬ë¶„ì½”ë“œëª…'] = df['ìƒê¶Œêµ¬ë¶„ì½”ë“œ'].map(code_name_dicts['ìƒê¶Œêµ¬ë¶„ì½”ë“œ'])
    if 'ìƒê¶Œì½”ë“œ' in df.columns:
        df['ìƒê¶Œì½”ë“œëª…'] = df['ìƒê¶Œì½”ë“œ'].map(code_name_dicts['ìƒê¶Œì½”ë“œ'])
    if 'ì„œë¹„ìŠ¤ì—…ì¢…ì½”ë“œ' in df.columns:
        df['ì„œë¹„ìŠ¤ì—…ì¢…ì½”ë“œëª…'] = df['ì„œë¹„ìŠ¤ì—…ì¢…ì½”ë“œ'].map(code_name_dicts['ì„œë¹„ìŠ¤ì—…ì¢…ì½”ë“œ'])
    return df

MODEL_PATHS = {
    "CatBoost": r"SKN13-2nd-4TEAM/ì‚°ì¶œë¬¼/best/cb_model.joblib",
    "Random Forest": r"SKN13-2nd-4TEAM/ì‚°ì¶œë¬¼/best/rf_model.joblib"
}

DROP_COLS = [
    'ìƒê¶Œ_êµ¬ë¶„_ì½”ë“œ', 'ìƒê¶Œ_êµ¬ë¶„_ì½”ë“œ_ëª…', 'ìƒê¶Œ_ì½”ë“œ', 'ìƒê¶Œ_ì½”ë“œ_ëª…',
    'ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ', 'ì„œë¹„ìŠ¤_ì—…ì¢…_ì½”ë“œ_ëª…',
    'ì£¼ì¤‘_ë§¤ì¶œ_ê±´ìˆ˜', 'ì£¼ë§_ë§¤ì¶œ_ê±´ìˆ˜',
    'ì›”ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'í™”ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'ìˆ˜ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'ëª©ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'ê¸ˆìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜',
    'í† ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'ì¼ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜',
    'ì‹œê°„ëŒ€_ê±´ìˆ˜~06_ë§¤ì¶œ_ê±´ìˆ˜', 'ì‹œê°„ëŒ€_ê±´ìˆ˜~11_ë§¤ì¶œ_ê±´ìˆ˜', 'ì‹œê°„ëŒ€_ê±´ìˆ˜~14_ë§¤ì¶œ_ê±´ìˆ˜',
    'ì‹œê°„ëŒ€_ê±´ìˆ˜~17_ë§¤ì¶œ_ê±´ìˆ˜', 'ì‹œê°„ëŒ€_ê±´ìˆ˜~21_ë§¤ì¶œ_ê±´ìˆ˜', 'ì‹œê°„ëŒ€_ê±´ìˆ˜~24_ë§¤ì¶œ_ê±´ìˆ˜',
    'ë‚¨ì„±_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—¬ì„±_ë§¤ì¶œ_ê±´ìˆ˜',
    'ì—°ë ¹ëŒ€_10_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—°ë ¹ëŒ€_40_ë§¤ì¶œ_ê±´ìˆ˜',
    'ì—°ë ¹ëŒ€_50_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—°ë ¹ëŒ€_60_ì´ìƒ_ë§¤ì¶œ_ê±´ìˆ˜'
]

uploaded_file = st.file_uploader("ğŸ“‚ ì˜ˆì¸¡í•  CSV íŒŒì¼", type=["csv"])

if uploaded_file:
    test_df = pd.read_csv(uploaded_file)
    st.success("âœ… CSV ì—…ë¡œë“œ ì™„ë£Œ. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ì˜ˆì¸¡ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    test_df.drop(columns=DROP_COLS, errors='ignore', inplace=True)

    model_infos = []
    for name, path in MODEL_PATHS.items():
        model, encoders, features, metrics = load_model_and_metadata(path)
        model_infos.append({
            "name": metrics.get("model_name", name),
            "model": model,
            "encoders": encoders,
            "features": features,
            "f1": metrics.get("f1_score", 0),
            "recall": metrics.get("recall", 0)
        })

    st.subheader("ğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
    st.table(pd.DataFrame({
        "ëª¨ë¸ ì´ë¦„": [m["name"] for m in model_infos],
        "F1 Score": [f"{m['f1']:.4f}" for m in model_infos],
        "Recall": [f"{m['recall']:.4f}" for m in model_infos]
    }))

    best_model_info = sorted(model_infos, key=lambda x: (x['recall'], x['f1']), reverse=True)[0]
    st.success(f"âœ… **{best_model_info['name']}** ëª¨ë¸ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")

    test_df_aligned = align_columns(test_df, best_model_info["features"])
    missing_cols = set(best_model_info["features"]) - set(test_df_aligned.columns)
    for col in missing_cols:
        test_df_aligned[col] = 0

    test_X = test_df_aligned[best_model_info["features"]].copy()
    test_X_scaled = preprocess(test_X, best_model_info["encoders"])

    with st.spinner("ğŸ“Š ì˜ˆì¸¡ ì§„í–‰ ì¤‘..."):
        model = best_model_info["model"]
        pred = model.predict(test_X_scaled)
        proba = model.predict_proba(test_X_scaled)[:, 1]
        test_df['íì—…ì˜ˆì¸¡'] = pred
        test_df['íì—…í™•ë¥ (%)'] = (proba * 100).round(2)
        test_df = add_korean_names(test_df)

    st.subheader("ğŸ“‹ ì˜ˆì¸¡ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")

    filter_cols = [
        ('ìƒê¶Œêµ¬ë¶„ì½”ë“œëª…', 'ìƒê¶Œ êµ¬ë¶„ ì„ íƒ'),
        ('ìƒê¶Œì½”ë“œëª…', 'ìƒê¶Œ ì„ íƒ'),
        ('ì„œë¹„ìŠ¤ì—…ì¢…ì½”ë“œëª…', 'ì—…ì¢… ì„ íƒ')
    ]

    for col, label in filter_cols:
        if col in test_df.columns:
            options = sorted(test_df[col].dropna().unique())
            selected = st.selectbox(f"ğŸ” {label}", ['ì „ì²´'] + options, key=col)
            if selected != 'ì „ì²´':
                test_df = test_df[test_df[col] == selected]

    st.dataframe(test_df.head(20))

    st.subheader("ğŸ” ì¸ì‚¬ì´íŠ¸ ì»¬ëŸ¼ ì„ íƒ")
    column_options = {
        "ìƒê¶Œ êµ¬ë¶„ ì½”ë“œëª…": "ìƒê¶Œêµ¬ë¶„ì½”ë“œëª…",
        "ìƒê¶Œ ì½”ë“œëª…": "ìƒê¶Œì½”ë“œëª…",
        "ì„œë¹„ìŠ¤ ì—…ì¢… ì½”ë“œëª…": "ì„œë¹„ìŠ¤ì—…ì¢…ì½”ë“œëª…"
    }
    selected_cols = [col for label, col in column_options.items() if col in test_df.columns and st.checkbox(label, value=True)]

    st.subheader("ğŸ’¡ ì„ íƒí•œ ì»¬ëŸ¼ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸")
    with st.expander("ğŸ“Œ íì—… ìœ„í—˜ ë†’ì€ ê·¸ë£¹ ë¶„ì„"):
        if selected_cols:
            grouped_df = test_df.groupby(selected_cols)['íì—…í™•ë¥ (%)'].mean().reset_index()
            # í‰ê·  íì—…í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ì¡°í•© ì°¾ê¸°
            top_row = grouped_df.sort_values(by='íì—…í™•ë¥ (%)', ascending=False).iloc[0]

            # ì¡°í•© ë¬¸ìì—´ ìƒì„±
            combo_desc = ", ".join([f"{col} **{top_row[col]}**" for col in selected_cols])
            prob_value = top_row['íì—…í™•ë¥ (%)']

            st.markdown(f"- {combo_desc}ì˜ í‰ê·  íì—… í™•ë¥ ì€ **{prob_value:.2f}%**ì…ë‹ˆë‹¤.")
        else:
            st.info("ì¸ì‚¬ì´íŠ¸ë¥¼ ë³´ê¸° ìœ„í•´ í•˜ë‚˜ ì´ìƒì˜ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”.")


    display_cols = selected_cols + ['íì—…ì˜ˆì¸¡', 'íì—…í™•ë¥ (%)']
    filtered_df = test_df[display_cols] if selected_cols else test_df[['íì—…ì˜ˆì¸¡', 'íì—…í™•ë¥ (%)']]
    st.dataframe(filtered_df.head(20))

    if st.checkbox("ğŸ” Feature ì¤‘ìš”ë„ ë³´ê¸°"):
        feat_imp = pd.Series(model.feature_importances_, index=best_model_info["features"]).sort_values(ascending=False)
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(x=feat_imp.values[:15], y=feat_imp.index[:15], palette="viridis", ax=ax)
        ax.set_title("Feature Importance (Top 15)")
        st.pyplot(fig)

    if 'ìœ„ë„' in test_df.columns and 'ê²½ë„' in test_df.columns:
        st.subheader("ğŸ—ºï¸ íì—… ì˜ˆì¸¡ ì§€ë„")
        m = folium.Map(location=[test_df['ìœ„ë„'].mean(), test_df['ê²½ë„'].mean()], zoom_start=13)
        for _, row in test_df.iterrows():
            color = "red" if row['íì—…ì˜ˆì¸¡'] == 1 else "green"
            folium.CircleMarker(
                location=[row['ìœ„ë„'], row['ê²½ë„']],
                radius=5,
                popup=f"íì—…í™•ë¥ : {row['íì—…í™•ë¥ (%)']:.1f}%",
                color=color,
                fill=True,
                fill_opacity=0.7
            ).add_to(m)
        st_folium(m, width=700, height=500)
else:
    st.info("â¬†ï¸ ì™¼ìª½ì—ì„œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
